{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c8bfa8-566e-4937-b8bb-c7f2f7781778",
   "metadata": {},
   "source": [
    "# Patient Readmission Risk â€“ Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook explores the **Diabetes 130-US hospitals dataset** (UCI Machine Learning Repository) to prepare it for modeling patient readmission risk within 30 days.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c166985-c801-46e0-9654-d4d816bf8d01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the data folder\n",
    "df = pd.read_csv(\"../data/diabetic_data.csv\")\n",
    "\n",
    "# Show dataset dimensions\n",
    "print(\"Shape (rows, columns):\", df.shape)\n",
    "\n",
    "# Show first 5 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329329c6-7188-44d3-bdeb-0b1b0ff6ca4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadmit_30\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mdf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadmitted\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<30\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Check balance of the target\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadmit_30\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"readmit_30\"] = (df[\"readmitted\"] == \"<30\").astype(int)\n",
    "\n",
    "# Check balance of the target\n",
    "df[\"readmit_30\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f96b0-28c2-4958-bef0-e06725b6a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See column names and data types\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1ba04-773d-4f62-beec-ced0d9180b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1409849-332b-4330-84fc-4c95723dbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df == \"?\").sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba90c0-74c8-4f55-9b03-fa6614216d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"race\", \"gender\", \"age\", \"max_glu_serum\", \"A1Cresult\"]:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d03e1-2868-48b6-84c4-8c96931365a3",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning\n",
    "\n",
    "- Drop ID columns (`encounter_id`, `patient_nbr`) since they donâ€™t add predictive value.  \n",
    "- Replace `\"?\"` with `NaN` for proper missing value handling.  \n",
    "- Check which columns have the most missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c60612-7f1e-4877-bb55-304cf676f690",
   "metadata": {},
   "source": [
    "# 1) Drop identifiers and very low-value columns\n",
    "to_drop = [\n",
    "    \"encounter_id\", \"patient_nbr\",   # pure IDs\n",
    "    \"weight\",                         # ~95% missing\n",
    "    \"payer_code\",                     # very sparse\n",
    "    \"medical_specialty\",              # many missing, ultra-high cardinality\n",
    "    \"examide\", \"citoglipton\",         # almost all 'No'\n",
    "    \"readmitted\"                      # replaced by readmit_30\n",
    "]\n",
    "df = df.drop(columns=[c for c in to_drop if c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039d6e3-403a-42ab-bde4-5847ebcdc0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = df.replace(\"?\", np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c026f-7c88-437e-85e9-bb8bacc00068",
   "metadata": {},
   "source": [
    "df.isnull().sum().sort_values(ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01630c28-200c-4ad7-82d2-7c092bd91f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check where missing values are, then\n",
    "\n",
    "#Impute (fill) them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40baa9d0-cf08-4e80-83d7-5e77fbadb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047fb301-fb9b-4325-904f-1b5c491e7b0a",
   "metadata": {},
   "source": [
    "## Step 6: Train-Test Split\n",
    "\n",
    "- Separated features (`X`) and target (`y`).\n",
    "- Used an 80/20 split (train = 81,412 rows, test = 20,354 rows).\n",
    "- Stratified by target to preserve class balance in both sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94956d0-31af-4668-a74f-97900bd9813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = \"readmit_30\"\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.mean(), y_test.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389ffa7-fc21-4279-8b3a-89dd56a87c23",
   "metadata": {},
   "source": [
    "Hereâ€™s what each part of the output means:\n",
    "\n",
    "- **(81412, 42)** â†’ `X_train` has 81,412 rows (examples) and 42 columns (features).  \n",
    "  These rows are used to fit (train) the model.  \n",
    "\n",
    "- **(20354, 42)** â†’ `X_test` has 20,354 rows and the same 42 features.  \n",
    "  These rows are held out for evaluation.  \n",
    "\n",
    "- **0.1116 (train)** â†’ About 11.16% of patients in the training set are positive cases (`readmit_30 = 1`).  \n",
    "\n",
    "- **0.1116 (test)** â†’ About 11.16% of patients in the test set are positive.  \n",
    "\n",
    "ğŸ’¡ Because we used `stratify=y` in the split, the class balance (â‰ˆ11% positives) is preserved in both train and test. This is critical for imbalanced datasets â€” otherwise, you might end up with too few positive cases in one set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eef7fd-0418-47b1-aae8-1ff5cea48695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb4c99-3bff-44e2-8086-d51741e285ad",
   "metadata": {},
   "source": [
    "## imports the preprocessing and modeling tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b03600-313b-4c8f-b954-f8fed319eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical vs numeric features\n",
    "cat_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = X_train.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "cat_cols[:5], num_cols[:5]   # quick peek at a few\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a5212-e647-4007-9bad-68202cd40d9c",
   "metadata": {},
   "source": [
    "## Step 7: Preprocessing + Baseline Model (Logistic Regression)\n",
    "\n",
    "- Imported preprocessing tools (`ColumnTransformer`, `OneHotEncoder`, `Pipeline`) and model (`LogisticRegression`).\n",
    "- Identified **categorical** columns (strings/objects) and **numeric** columns (integers/floats).\n",
    "- Built a preprocessing pipeline:\n",
    "  - One-hot encode categorical variables.\n",
    "  - Pass numeric variables through unchanged.\n",
    "- Chose Logistic Regression as the baseline model:\n",
    "  - Simple, interpretable, widely used in healthcare.\n",
    "  - Added `class_weight=\"balanced\"` to account for imbalanced data (~11% positives).\n",
    "- Trained the pipeline on the training set (`X_train`, `y_train`).\n",
    "- Evaluated performance on the test set using:\n",
    "  - **AUROC** (how well the model ranks positive vs negative cases).\n",
    "  - **AUPRC** (how well the model identifies positives in an imbalanced dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c1380-071c-4b46-a09a-e424f63ff839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = X_train.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"scaler\", StandardScaler())]), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),  # sparse by default\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    force_int_remainder_cols=False  # silences the future warning & adopts new behavior\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0302947-f26e-49c0-8fd9-d27fc766bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"saga\",            # handles large sparse designs better\n",
    "        max_iter=5000,            # give it room to converge\n",
    "        class_weight=\"balanced\",  # handle ~11% positives\n",
    "        n_jobs=-1,                # use all CPU cores\n",
    "        C=0.5                     # a bit more regularization; adjust if needed\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254450e-99f2-440f-9b77-45ef2de874d4",
   "metadata": {},
   "source": [
    "## Step 8: Model Evaluation\n",
    "\n",
    "- Evaluating the fitted Logistic Regression pipeline on the test set (20,354 rows).\n",
    "- Metrics:\n",
    "  - **AUROC** (Area Under ROC Curve): measures ability to rank positives vs negatives.\n",
    "  - **AUPRC** (Area Under Precision-Recall Curve): better for imbalanced data, shows how well we identify positives.\n",
    "- Both metrics are important:\n",
    "  - AUROC tells us general ranking performance.\n",
    "  - AUPRC tells us how useful the model is in catching the rare 11% positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80c5b2-a782-483b-b4b9-47320f67c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Predict probabilities for test set\n",
    "p_test = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "auroc = roc_auc_score(y_test, p_test)\n",
    "auprc = average_precision_score(y_test, p_test)\n",
    "\n",
    "print(\"AUROC:\", round(auroc, 3))\n",
    "print(\"AUPRC:\", round(auprc, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af75a2-a6ec-4376-a96f-ad76201d7edb",
   "metadata": {},
   "source": [
    "## Step 8: Model Evaluation\n",
    "\n",
    "- Evaluated pipeline using AUROC and AUPRC on the test set (20,354 rows).\n",
    "- **AUROC = 0.646** â†’ Model is moderately effective at ranking positives vs negatives.\n",
    "- **AUPRC = 0.205** â†’ ~2Ã— better than random (baseline prevalence = 0.11).\n",
    "- Interpretation:\n",
    "  - Logistic Regression provides a solid baseline.\n",
    "  - Model is better than chance but not highly accurate â†’ suggests trying stronger models (e.g., XGBoost).\n",
    "- Next:\n",
    "  - Experiment with thresholding (e.g., flag top 10% high-risk patients) to calculate precision/recall.\n",
    "  - Compare with tree-based models to improve AUROC/AUPRC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be0f569-4f11-41a4-8bcf-c3df02e43706",
   "metadata": {},
   "source": [
    "## Step 9: Thresholding Predictions\n",
    "\n",
    "- Logistic Regression outputs probabilities, but in practice we need to choose a threshold to classify patients as \"at risk.\"\n",
    "- Instead of the default threshold (0.5), we flagged the **top 10% of patients by predicted risk** as positives.\n",
    "- This approach simulates how hospitals might focus limited resources on the riskiest patients.\n",
    "\n",
    "### Why this matters\n",
    "- **Precision** â†’ Of the patients we flagged, how many truly were readmitted within 30 days?  \n",
    "- **Recall** â†’ Of all patients who were actually readmitted, how many did we catch?  \n",
    "- **F1-score** â†’ Balance of precision and recall.  \n",
    "\n",
    "### Key learning\n",
    "- Choosing a threshold depends on business/clinical priorities:\n",
    "  - **High precision** â†’ fewer false alarms (good when resources are expensive).  \n",
    "  - **High recall** â†’ catch more true cases (good when missing a case is very costly).  \n",
    "- Evaluating at the top 10% risk level shows how the model could be used in a real-world hospital setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f68411-ca21-4551-9512-da0205c8fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define threshold as the 90th percentile of predicted risk scores\n",
    "thresh = np.quantile(p_test, 0.90)\n",
    "\n",
    "# Classify as positive if risk >= threshold\n",
    "pred_10 = (p_test >= thresh).astype(int)\n",
    "\n",
    "print(\"Threshold (90th percentile risk):\", round(thresh, 4))\n",
    "print(classification_report(y_test, pred_10, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0820e5-0992-4249-9809-1ccbe9323668",
   "metadata": {},
   "source": [
    "### Step 9: Thresholding Results (Top 10% Risk Patients)\n",
    "\n",
    "- **Threshold chosen:** 0.6536 (90th percentile of predicted probabilities).  \n",
    "- **Results at this threshold:**\n",
    "  - Precision (positives) = 0.242 â†’ Of the patients flagged as high risk, ~24% were truly readmitted.  \n",
    "  - Recall (positives) = 0.217 â†’ The model identified ~22% of all actual readmissions.  \n",
    "  - F1-score = 0.228 â†’ Balance of precision and recall is modest.  \n",
    "  - Accuracy = 0.837, but accuracy is less meaningful with imbalance.\n",
    "\n",
    "### Interpretation\n",
    "- The model does focus on a smaller high-risk group: flagged patients are more than **2Ã— as likely** to be readmitted compared to baseline prevalence (24% vs 11%).  \n",
    "- However, recall is limited: ~78% of true readmissions were missed at this threshold.  \n",
    "- This highlights the **precisionâ€“recall trade-off**:\n",
    "  - Raising threshold â†’ fewer flagged, higher precision, lower recall.  \n",
    "  - Lowering threshold â†’ more flagged, lower precision, higher recall.  \n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different thresholds (e.g., top 20%, 30%) to explore trade-offs.  \n",
    "- Try stronger models (e.g., XGBoost) to improve AUROC/AUPRC.  \n",
    "- Consider calibration to improve probability estimates.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f39d4e-bd3f-493d-a095-3e66bbaa96e4",
   "metadata": {},
   "source": [
    "## Step 10: Stronger Model (XGBoost)\n",
    "\n",
    "- XGBoost (Extreme Gradient Boosting) is a tree-based ensemble method:\n",
    "  - Builds many decision trees sequentially, each correcting errors of the last.\n",
    "  - Handles non-linear relationships better than Logistic Regression.\n",
    "  - Robust to imbalanced datasets when using `scale_pos_weight`.\n",
    "- Expectation: Better AUROC and AUPRC compared to Logistic Regression.\n",
    "- Goal: Train and evaluate XGBoost with the same preprocessing pipeline for a fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b74ceb-b524-4d67-9bd7-8c6e15d8814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "print(\"XGBoost is working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d623f3-ec8d-49bd-bd6d-41297d86dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Exec:\", sys.executable)\n",
    "import numpy as np, pandas as pd\n",
    "import sklearn\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"xgboost:\", xgb.__version__)\n",
    "except Exception as e:\n",
    "    print(\"xgboost import error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7a697-c939-4cf4-b772-df2965cf61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Imports ====\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ==== Load data ====\n",
    "df = pd.read_csv(\"../data/diabetic_data.csv\")\n",
    "\n",
    "# Target\n",
    "df[\"readmit_30\"] = (df[\"readmitted\"] == \"<30\").astype(int)\n",
    "\n",
    "# Drop low-value columns\n",
    "to_drop = [\n",
    "    \"encounter_id\", \"patient_nbr\",\n",
    "    \"weight\", \"payer_code\", \"medical_specialty\",\n",
    "    \"examide\", \"citoglipton\",\n",
    "    \"readmitted\",\n",
    "]\n",
    "df = df.drop(columns=[c for c in to_drop if c in df.columns])\n",
    "\n",
    "# Normalize missing markers and simple imputations (fast baseline)\n",
    "df = df.replace(\"?\", np.nan)\n",
    "\n",
    "num_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "if \"readmit_30\" in num_cols: num_cols.remove(\"readmit_30\")\n",
    "\n",
    "for c in num_cols:\n",
    "    df[c] = df[c].astype(float).fillna(df[c].median())\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].fillna(\"Unknown\")\n",
    "\n",
    "# Train/test split (stratified)\n",
    "X = df.drop(columns=[\"readmit_30\"])\n",
    "y = df[\"readmit_30\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess: scale numeric, OHE categorical (sparse)\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"scaler\", StandardScaler())]), X_train.select_dtypes(exclude=\"object\").columns.tolist()),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), X_train.select_dtypes(include=\"object\").columns.tolist()),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "\n",
    "# ==== Quick sanity model (LogReg) on a subset for speed ====\n",
    "subset = min(10000, len(X_train))\n",
    "X_tr_sub = X_train.sample(subset, random_state=42)\n",
    "y_tr_sub = y_train.loc[X_tr_sub.index]\n",
    "\n",
    "logreg = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"saga\", max_iter=2000, class_weight=\"balanced\", n_jobs=-1, C=0.7\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg.fit(X_tr_sub, y_tr_sub)\n",
    "p_test_lr = logreg.predict_proba(X_test)[:, 1]\n",
    "print(\"Sanity AUROC (LogReg subset):\", round(roc_auc_score(y_test, p_test_lr), 3))\n",
    "print(\"Sanity AUPRC (LogReg subset):\", round(average_precision_score(y_test, p_test_lr), 3))\n",
    "\n",
    "# ==== Try XGBoost if available; else fall back to HistGradientBoosting ====\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    model = Pipeline(steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"model\", XGBClassifier(\n",
    "            n_estimators=300, max_depth=4, learning_rate=0.08,\n",
    "            subsample=0.9, colsample_bytree=0.9, eval_metric=\"logloss\",\n",
    "            scale_pos_weight=(y_train.value_counts()[0]/y_train.value_counts()[1])\n",
    "        ))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    p_test = model.predict_proba(X_test)[:, 1]\n",
    "    print(\"XGB AUROC:\", round(roc_auc_score(y_test, p_test), 3))\n",
    "    print(\"XGB AUPRC:\", round(average_precision_score(y_test, p_test), 3))\n",
    "    chosen = \"XGBoost\"\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not available, using HistGradientBoosting:\", e)\n",
    "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "    model = Pipeline(steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"model\", HistGradientBoostingClassifier(\n",
    "            learning_rate=0.08, max_depth=4, max_iter=300,\n",
    "            class_weight={0:1, 1:(y_train.value_counts()[0]/y_train.value_counts()[1])}\n",
    "        ))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    # HGB returns decision_function as proba only if predict_proba exists; it does in recent sklearn\n",
    "    p_test = model.predict_proba(X_test)[:, 1]\n",
    "    print(\"HGB AUROC:\", round(roc_auc_score(y_test, p_test), 3))\n",
    "    print(\"HGB AUPRC:\", round(average_precision_score(y_test, p_test), 3))\n",
    "\n",
    "# Threshold @ top 10% for the chosen model\n",
    "import numpy as np\n",
    "thresh = np.quantile(p_test, 0.90)\n",
    "pred_10 = (p_test >= thresh).astype(int)\n",
    "print(\"Threshold (90th percentile):\", round(thresh, 4))\n",
    "print(classification_report(y_test, pred_10, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6a676-e6c1-40a5-8625-16acdc52ed9e",
   "metadata": {},
   "source": [
    "## Step 10: Stronger Model (XGBoost) â€” Results\n",
    "\n",
    "**Metrics (test set):**\n",
    "- **AUROC:** 0.682  \n",
    "- **AUPRC:** 0.232  \n",
    "- (Baseline Logistic Regression earlier: AUROC 0.646, AUPRC 0.205)\n",
    "\n",
    "**Thresholded evaluation (top 10% highest risk):**\n",
    "- **Precision:** 0.272  \n",
    "- **Recall:** 0.244  \n",
    "- **Accuracy:** 0.843  \n",
    "- Interpretation: The flagged cohort is ~2.5Ã— the baseline prevalence (27% vs 11%), and we capture ~24% of true readmissions at this operating point.\n",
    "\n",
    "**Takeaways:**\n",
    "- XGBoost improves ranking and early-warning utility over Logistic Regression.\n",
    "- Further gains likely from threshold tuning, hyperparameter search, and feature engineering (e.g., diagnosis grouping, medication aggregates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161dba0-215e-4f30-8a07-5f09e9b1b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# (Re)build the pipeline with a non-conflicting name\n",
    "xgb_pipe = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        eval_metric=\"logloss\",\n",
    "        scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1])\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit if needed (skip if you already have a fitted pipeline under a different name)\n",
    "xgb_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Ensure the reports folder exists\n",
    "import os\n",
    "os.makedirs(\"../reports\", exist_ok=True)\n",
    "\n",
    "# Save the fitted pipeline\n",
    "import joblib\n",
    "joblib.dump(xgb_pipe, \"../reports/readmission_xgb_pipeline.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e2773-456b-49af-af55-041d1c2f27d9",
   "metadata": {},
   "source": [
    "## Step 11: Model Saving & Reuse\n",
    "\n",
    "- Saved the trained XGBoost pipeline to `../reports/readmission_xgb_pipeline.joblib`.\n",
    "- The file contains the **entire pipeline**:\n",
    "  - Preprocessing (scaling + one-hot encoding).\n",
    "  - Trained XGBoost model.\n",
    "- Benefits:\n",
    "  - Can reload later without retraining.\n",
    "  - Ensures consistent preprocessing and model logic.\n",
    "- Verified by reloading and confirming AUROC/AUPRC match previous results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c030a68-fe7a-44e6-875c-ba1c522390c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved pipeline\n",
    "loaded_model = joblib.load(\"../reports/readmission_xgb_pipeline.joblib\")\n",
    "\n",
    "# Sanity check: evaluate on test set\n",
    "p_loaded = loaded_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Reloaded AUROC:\", round(roc_auc_score(y_test, p_loaded), 3))\n",
    "print(\"Reloaded AUPRC:\", round(average_precision_score(y_test, p_loaded), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9257022-0481-4ace-a84e-9ccbc6bd2404",
   "metadata": {},
   "source": [
    "### Step 11: Model Saving & Verification\n",
    "\n",
    "- Reloaded the saved pipeline (`../reports/readmission_xgb_pipeline.joblib`) using `joblib.load`.  \n",
    "- Evaluated the reloaded model on the test set:  \n",
    "  - AUROC â‰ˆ 0.682  \n",
    "  - AUPRC â‰ˆ 0.232  \n",
    "- Results match the original Step 10 evaluation, confirming that:\n",
    "  - The entire pipeline (preprocessing + XGBoost model) was saved successfully.  \n",
    "  - Reloading works as expected and can be used for future predictions without retraining.  \n",
    "\n",
    "**Key takeaway:**  \n",
    "Saving models with `joblib` is essential for deployment and reproducibility. It ensures that preprocessing and model logic stay consistent across training, testing, and future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b9720-96c4-4f16-b675-ca3a0180c91f",
   "metadata": {},
   "source": [
    "## Step 12: Demo Predictions (Deployment-Style)\n",
    "\n",
    "- Goal: Show how the saved model can be reused to score **new patient data** without retraining.\n",
    "- Process:\n",
    "  1. Load the saved pipeline using `joblib.load`.\n",
    "  2. Create a small sample input (new patient encounter) with the same feature structure as the training data.\n",
    "  3. Use `.predict_proba()` to generate readmission risk probabilities.\n",
    "  4. Interpret the output:\n",
    "     - A probability close to 1 â†’ high risk of 30-day readmission.\n",
    "     - A probability close to 0 â†’ low risk.\n",
    "- Importance:\n",
    "  - Demonstrates the model is **deployment-ready**.\n",
    "  - Clinicians, analysts, or downstream systems could plug in real patient data to get a risk score.\n",
    "  - Closing the loop: from raw data â†’ trained model â†’ saved pipeline â†’ real-world predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbf657-8aa4-49a9-9bf4-543bd3ce4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, joblib\n",
    "\n",
    "# Load saved pipeline\n",
    "loaded_model = joblib.load(\"../reports/readmission_xgb_pipeline.joblib\")\n",
    "\n",
    "# Get original raw feature columns from the ColumnTransformer\n",
    "ct = loaded_model.named_steps[\"prep\"]\n",
    "cat_cols, num_cols = [], []\n",
    "for name, trans, cols in ct.transformers_:\n",
    "    if name == \"cat\":\n",
    "        cat_cols = list(cols)\n",
    "    elif name == \"num\":\n",
    "        num_cols = list(cols)\n",
    "\n",
    "feature_cols = num_cols + cat_cols  # raw input order\n",
    "\n",
    "# Build defaults then overwrite with your demo values\n",
    "demo_data = {c: (\"Unknown\" if c in cat_cols else 0) for c in feature_cols}\n",
    "demo_data.update({\n",
    "    \"race\": \"Caucasian\",\n",
    "    \"gender\": \"Female\",\n",
    "    \"age\": \"[60-70)\",\n",
    "    \"admission_type_id\": 1,\n",
    "    \"discharge_disposition_id\": 1,\n",
    "    \"admission_source_id\": 7,\n",
    "    \"time_in_hospital\": 5,\n",
    "    \"num_lab_procedures\": 45,\n",
    "    \"num_procedures\": 1,\n",
    "    \"num_medications\": 12,\n",
    "    \"number_diagnoses\": 4,\n",
    "    \"diag_1\": \"250.00\",\n",
    "    \"diag_2\": \"401.9\",\n",
    "    \"diag_3\": \"414.01\",\n",
    "    \"max_glu_serum\": \"None\",\n",
    "    \"A1Cresult\": \">7\",\n",
    "    \"metformin\": \"No\",\n",
    "    \"insulin\": \"Up\",\n",
    "    \"change\": \"Ch\",\n",
    "    \"diabetesMed\": \"Yes\"\n",
    "})\n",
    "\n",
    "demo_patient = pd.DataFrame([demo_data], columns=feature_cols)\n",
    "risk_score = loaded_model.predict_proba(demo_patient)[:, 1][0]\n",
    "print(\"Predicted 30-day readmission risk:\", f\"{risk_score:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71448465-3624-4224-8e8a-db93e009388f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhisekkumar/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1) Grab pieces from the saved pipeline\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprep\u001b[39m\u001b[38;5;124m\"\u001b[39m]          \u001b[38;5;66;03m# ColumnTransformer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]        \u001b[38;5;66;03m# XGBClassifier\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 2) Transform the demo row to the encoded feature space\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Text-only SHAP explanation for the demo patient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "# 1) Grab pieces from the saved pipeline\n",
    "prep = loaded_model.named_steps[\"prep\"]          # ColumnTransformer\n",
    "model = loaded_model.named_steps[\"model\"]        # XGBClassifier\n",
    "\n",
    "# 2) Transform the demo row to the encoded feature space\n",
    "Xp = prep.transform(demo_patient)\n",
    "\n",
    "# 3) Get output feature names (after OHE + scaling)\n",
    "feat_names = prep.get_feature_names_out()\n",
    "\n",
    "# 4) Compute SHAP values for this single row (tree explainer is fast)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_vals = explainer.shap_values(Xp)            # shape: (n_samples, n_features)\n",
    "shap_row = np.asarray(shap_vals)[0]              # take first (and only) row\n",
    "\n",
    "# 5) Build a tidy table: feature, contribution (signed), |contribution|\n",
    "contrib = pd.DataFrame({\n",
    "    \"feature\": feat_names,\n",
    "    \"shap\": shap_row,\n",
    "    \"abs_shap\": np.abs(shap_row)\n",
    "}).sort_values(\"abs_shap\", ascending=False)\n",
    "\n",
    "# 6) Show top drivers up and down\n",
    "top_k = 10  # adjust to see more/less\n",
    "top_up = contrib[contrib[\"shap\"] > 0].head(top_k)[[\"feature\",\"shap\"]]\n",
    "top_down = contrib[contrib[\"shap\"] < 0].head(top_k)[[\"feature\",\"shap\"]]\n",
    "\n",
    "print(\"Top factors INCREASING risk:\")\n",
    "display(top_up)\n",
    "\n",
    "print(\"\\nTop factors DECREASING risk:\")\n",
    "display(top_down)\n",
    "\n",
    "# Optional: concise bullet list\n",
    "def bullets(df):\n",
    "    return [f\"{r.feature}: {r.shap:+.3f}\" for r in df.itertuples(index=False)]\n",
    "print(\"\\nQuick summary â†‘:\", bullets(top_up))\n",
    "print(\"Quick summary â†“:\", bullets(top_down))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e01b41-0d3e-4b29-97c0-9f805b309888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 30-day readmission risk: 36.3%\n",
      "\n",
      "Top factors INCREASING risk:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>shap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num__time_in_hospital</td>\n",
       "      <td>0.035460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num__num_medications</td>\n",
       "      <td>0.024733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>cat__diabetesMed_No</td>\n",
       "      <td>0.024201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>cat__max_glu_serum_Unknown</td>\n",
       "      <td>0.014354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>cat__metformin_No</td>\n",
       "      <td>0.013106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cat__age_[50-60)</td>\n",
       "      <td>0.011048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>cat__diag_1_786</td>\n",
       "      <td>0.010925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>cat__diag_3_250</td>\n",
       "      <td>0.009701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>cat__diag_1_486</td>\n",
       "      <td>0.009194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>cat__diag_2_285</td>\n",
       "      <td>0.007404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         feature      shap\n",
       "3          num__time_in_hospital  0.035460\n",
       "6           num__num_medications  0.024733\n",
       "2277         cat__diabetesMed_No  0.024201\n",
       "2203  cat__max_glu_serum_Unknown  0.014354\n",
       "2209           cat__metformin_No  0.013106\n",
       "25              cat__age_[50-60)  0.011048\n",
       "556              cat__diag_1_786  0.010925\n",
       "1519             cat__diag_3_250  0.009701\n",
       "348              cat__diag_1_486  0.009194\n",
       "861              cat__diag_2_285  0.007404"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top factors DECREASING risk:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>shap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num__number_inpatient</td>\n",
       "      <td>-0.319334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num__discharge_disposition_id</td>\n",
       "      <td>-0.206420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>num__number_diagnoses</td>\n",
       "      <td>-0.117580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>cat__A1Cresult_Unknown</td>\n",
       "      <td>-0.054839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num__number_emergency</td>\n",
       "      <td>-0.025570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>cat__acarbose_No</td>\n",
       "      <td>-0.024632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>cat__insulin_Down</td>\n",
       "      <td>-0.015481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>cat__diag_1_428</td>\n",
       "      <td>-0.013505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>cat__glyburide_No</td>\n",
       "      <td>-0.007645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cat__age_[70-80)</td>\n",
       "      <td>-0.005954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            feature      shap\n",
       "9             num__number_inpatient -0.319334\n",
       "1     num__discharge_disposition_id -0.206420\n",
       "10            num__number_diagnoses -0.117580\n",
       "2207         cat__A1Cresult_Unknown -0.054839\n",
       "8             num__number_emergency -0.025570\n",
       "2248               cat__acarbose_No -0.024632\n",
       "2259              cat__insulin_Down -0.015481\n",
       "299                 cat__diag_1_428 -0.013505\n",
       "2234              cat__glyburide_No -0.007645\n",
       "27                 cat__age_[70-80) -0.005954"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quick summary â†‘: ['num__time_in_hospital: +0.035', 'num__num_medications: +0.025', 'cat__diabetesMed_No: +0.024', 'cat__max_glu_serum_Unknown: +0.014', 'cat__metformin_No: +0.013', 'cat__age_[50-60): +0.011', 'cat__diag_1_786: +0.011', 'cat__diag_3_250: +0.010', 'cat__diag_1_486: +0.009', 'cat__diag_2_285: +0.007']\n",
      "Quick summary â†“: ['num__number_inpatient: -0.319', 'num__discharge_disposition_id: -0.206', 'num__number_diagnoses: -0.118', 'cat__A1Cresult_Unknown: -0.055', 'num__number_emergency: -0.026', 'cat__acarbose_No: -0.025', 'cat__insulin_Down: -0.015', 'cat__diag_1_428: -0.014', 'cat__glyburide_No: -0.008', 'cat__age_[70-80): -0.006']\n"
     ]
    }
   ],
   "source": [
    "# === Self-contained SHAP explainability for one demo patient ===\n",
    "import numpy as np, pandas as pd, joblib, shap\n",
    "\n",
    "# 1) Load the saved pipeline\n",
    "loaded_model = joblib.load(\"../reports/readmission_xgb_pipeline.joblib\")\n",
    "prep = loaded_model.named_steps[\"prep\"]     # ColumnTransformer\n",
    "model = loaded_model.named_steps[\"model\"]   # XGBClassifier\n",
    "\n",
    "# 2) Rebuild the raw feature list from the fitted ColumnTransformer\n",
    "cat_cols, num_cols = [], []\n",
    "for name, trans, cols in prep.transformers_:\n",
    "    if name == \"cat\":\n",
    "        cat_cols = list(cols)\n",
    "    elif name == \"num\":\n",
    "        num_cols = list(cols)\n",
    "feature_cols = num_cols + cat_cols\n",
    "\n",
    "# 3) Create a demo patient with safe defaults, then overwrite key values\n",
    "demo_data = {c: (\"Unknown\" if c in cat_cols else 0) for c in feature_cols}\n",
    "demo_data.update({\n",
    "    \"race\": \"Caucasian\",\n",
    "    \"gender\": \"Female\",\n",
    "    \"age\": \"[60-70)\",\n",
    "    \"admission_type_id\": 1,\n",
    "    \"discharge_disposition_id\": 1,\n",
    "    \"admission_source_id\": 7,\n",
    "    \"time_in_hospital\": 5,\n",
    "    \"num_lab_procedures\": 45,\n",
    "    \"num_procedures\": 1,\n",
    "    \"num_medications\": 12,\n",
    "    \"number_diagnoses\": 4,\n",
    "    \"diag_1\": \"250.00\",\n",
    "    \"diag_2\": \"401.9\",\n",
    "    \"diag_3\": \"414.01\",\n",
    "    \"max_glu_serum\": \"None\",\n",
    "    \"A1Cresult\": \">7\",\n",
    "    \"metformin\": \"No\",\n",
    "    \"insulin\": \"Up\",\n",
    "    \"change\": \"Ch\",\n",
    "    \"diabetesMed\": \"Yes\"\n",
    "})\n",
    "demo_patient = pd.DataFrame([demo_data], columns=feature_cols)\n",
    "\n",
    "# 4) Predict risk (sanity check)\n",
    "risk = float(loaded_model.predict_proba(demo_patient)[:, 1][0])\n",
    "\n",
    "# 5) Transform to model input space and compute SHAP\n",
    "Xp = prep.transform(demo_patient)\n",
    "feat_names = prep.get_feature_names_out()\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_vals = explainer.shap_values(Xp)\n",
    "shap_row = np.asarray(shap_vals)[0]\n",
    "\n",
    "# 6) Rank features by contribution\n",
    "contrib = pd.DataFrame({\n",
    "    \"feature\": feat_names,\n",
    "    \"shap\": shap_row,\n",
    "    \"abs_shap\": np.abs(shap_row)\n",
    "}).sort_values(\"abs_shap\", ascending=False)\n",
    "\n",
    "top_k = 10\n",
    "top_up = contrib[contrib[\"shap\"] > 0].head(top_k)[[\"feature\",\"shap\"]]\n",
    "top_down = contrib[contrib[\"shap\"] < 0].head(top_k)[[\"feature\",\"shap\"]]\n",
    "\n",
    "print(f\"Predicted 30-day readmission risk: {risk:.1%}\\n\")\n",
    "print(\"Top factors INCREASING risk:\")\n",
    "display(top_up)\n",
    "print(\"\\nTop factors DECREASING risk:\")\n",
    "display(top_down)\n",
    "\n",
    "def bullets(df):\n",
    "    return [f\"{r.feature}: {r.shap:+.3f}\" for r in df.itertuples(index=False)]\n",
    "print(\"\\nQuick summary â†‘:\", bullets(top_up))\n",
    "print(\"Quick summary â†“:\", bullets(top_down))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fff9d-de65-4e90-a091-3ee68f57cb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
